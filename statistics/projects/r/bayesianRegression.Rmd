---
title: "Bayesian regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Libraries
library(BAS)
library(ggplot2)
```

# Data understanding

## Load data
```{r}
data(bodyfat)
```

## Exploratory data analysis
```{r}
summary(bodyfat)
```

# Linear Regression model
To construct bayesian models using linear regression I will use **reference priors**, so a good way to get estimate parameters is using "Frequentist Ordinary Least Square (OLS)", because OLS has the same results.

Conditions:

* Residuals should be independent and identically distributed and normal distributed with mean zero and unknown variance.

## Simple linear regression
```{r}
# Use OLS method to calculate estimate parameters
bodyfat.lm <- lm(Bodyfat ~ Abdomen, data = bodyfat)
summary(bodyfat.lm)
```

```{r}
beta <- coef(bodyfat.lm)

# Visualize regression line on the scatter plot
library(ggplot2)
ggplot(data = bodyfat, aes(x = Abdomen, y = Bodyfat)) +
  geom_point(color = "blue") +
  geom_abline(intercept = beta[1], slope = beta[2], size = 1) +
  xlab("abdomen circumference (cm)") 
```

### Use reference prior to calculate posterior distribution of estimate parameters
Verify Conditions of resudials:

The residuals and fitted values should be uncorrelated, and the expected value of the residuals is zero
```{r}
# Combine residuals and fitted values into a data frame
result = data.frame(fitted_values = fitted.values(bodyfat.lm),
                    residuals = residuals(bodyfat.lm))

# Load library and plot residuals versus fitted values
library(ggplot2)
ggplot(data = result, aes(x = fitted_values, y = residuals)) +
  geom_point(pch = 1, size = 2) + 
  geom_abline(intercept = 0, slope = 0) + 
  xlab(expression(paste("fitted value ", widehat(Bodyfat)))) + 
  ylab("residuals")
```

Check normality.
```{r}
plot(bodyfat.lm, which = 2)
```

Both conditions are met, so linear regression is a reasonable approximation.

#### Calculate credible intervals
Under reference prior distribution, credible intervals are **numerically equivalent** to the confidence intervals from OLS.

```{r}
summary(bodyfat.lm)$coef
```

Interpretation: based on the data, we believe that there is 95% chance that body fat will increase by 5.75% up to 6.88% for every additional 10 centimeter increase in the waist circumference.
```{r}
output = summary(bodyfat.lm)$coef[, 1:2]

out = cbind(output, confint(bodyfat.lm))
colnames(out) = c("posterior mean", "posterior std", "2.5", "97.5")
round(out, 2)
```

#### Credible Intervals for the mean and prediction

```{r}
# Create new data
new_x <- seq(min(bodyfat$Abdomen), max(bodyfat$Abdomen), length.out = 100)
new_x[1:5]
```

##### Mean
Calculate credible interval for the mean based on each observation.
```{r}
ymean <- predict(bodyfat.lm, newdata = data.frame(Abdomen = new_x), interval = "confidence", level = 0.95)
ymean[1:5, ]
```

##### New prediction
Calculate credible interval for the new prediction based on each observation.
```{r}
ypred = predict(bodyfat.lm, newdata = data.frame(Abdomen = new_x), interval = "prediction", level = 0.95)
ypred[1:5, ]
```

## Multiple linear regression
Under reference prior like prior information, I have to calculate esimate parameters using OLS method.

```{r}
# Use OLS method to calculate estimate parameters
# BIC: the model is based on the non-informative reference prior.
# bestmodel = rep(1, 15): force the model to include all predictors.
bodyfat.bas <- bas.lm(Bodyfat ~ ., data = bodyfat, prior = 'BIC', modelprior = Bernoulli(1), bestmodel = rep(1, 15), n.models = 1)
bodyfat.bas
```

### Calculate posterior means and posterior standard deviations

```{r}
bodyfat.coef = coef(bodyfat.bas)
bodyfat.coef
```
Observations:

* **post p(B != 0)**: probability that a parameter can be non-zero. In this case is 1, because I forced it to include all parameters.

### Plot posterior distribution of all predictors based on OLS estimate parameter
```{r}
par(mfrow = c(4, 4), col.lab = "darkgrey", col.axis = "darkgrey", col = "darkgrey")
plot(bodyfat.coef, subset = 2:15, ask = F)
```


### Credible intervals for the parameters

```{r}
confint(bodyfat.coef, parm = 2:15)
```

```{r}
out = confint(bodyfat.coef)[, 1:2]  

# Extract the upper and lower bounds of the credible intervals
names = c("posterior mean", "posterior std", colnames(out))
out = cbind(bodyfat.coef$postmean, bodyfat.coef$postsd, out)
colnames(out) = names

round(out, 2)
```

Observation: all predictors, except "Density", include zero in their credible interval, that may mean that the model could be better with fewer predictors.

## Model selection
To get the best model, I will use Bayesian Information Criterion (BIC), that is one of the most popular criterias. In this case, the model with the smallest BIC is the best (parsimonious model). 

One way to calculate BIC is using frequentist OLS functions, because, under reference prior method, frequentist estimate parameters are numerically same.

### Backward Elimination with BIC
With this method I will start with all predictors, and I will remove one by one until getting the smallest BIC.
In OLS functions there are BIC parameter, but AIC (Akaike information criterion). However, AIC is numerically same that BIC, but using `k=log(n)`.

```{r}
# In bayesian aproximation AIC = BIC
# k = log(n)
# select the model with the smallest BIC

# Get number of observations
n <- nrow(bodyfat)

# Calculate frequentist OLS linear regression model
bodyfat.lm <- lm(Bodyfat ~ ., data = bodyfat)

bodyfat.step <- step(bodyfat.lm, k=log(n))
```

Showing the result, the parsimonious model has **BIC = 133.3**, where there are only two predictors: Chest and Density.

### Get the best BIC using BAS package 
This method does not use backward steps, but calculate log of marginal likelihood of each model, where the largest value corresponds to the smallest BIC.

```{r}
# Using BAS package
bodyfat.BIC = bas.lm(Bodyfat ~ ., data = bodyfat,
                 prior = "BIC", modelprior = uniform())

bodyfat.BIC
```

Find the model with the largest marginal likelihood by its index. Once the best model is identified, find the indexs of predictors that are part of that model.
In this case, the beest predictors are 1 and 7 index.
```{r}
# Find the index of the model with the largest logmarg
best = which.max(bodyfat.BIC$logmarg)

# Retreat the index of variables in the best model, with 0 as the index of the intercept
bestmodel = bodyfat.BIC$which[[best]]
bestmodel
```

In this code, I can convert predictors of the best model in a vector, where predictors with value one should be selected.
```{r}
# Create an indicator vector indicating which variables are used in the best model
bestgamma = rep(0, bodyfat.BIC$n.vars) 

# Create a 0 vector with the same dimension of the number of variables in the full model
bestgamma[bestmodel + 1] = 1  

# Change the indicator to 1 where variables are used
bestgamma
```

### Coefficient Estimates Under Reference Prior for Best BIC Model
Once indentified the best model, I can use it to calculate the posterior distribution of its variables.

```{r}
# Fit the best BIC model by imposing which variables to be used using the indicators
cog.bestBIC = bas.lm(Bodyfat ~ ., data = bodyfat,
                     prior = "BIC", n.models = 1,  # We only fit 1 model (the best model)
                     bestmodel = bestgamma,  # We use bestgamma to indicate variables 
                     modelprior = uniform())

# Retreat coefficients information
cog.coef = coef(cog.bestBIC)

# Retreat bounds of credible intervals
out = confint(cog.coef)[, 1:2]

# Combine results and construct summary table
coef.BIC = cbind(cog.coef$postmean, cog.coef$postsd, out)
names = c("post mean", "post sd", colnames(out))
colnames(coef.BIC) = names
coef.BIC
```

The function only calculated credible intervals of variables of the best model.

### Control model uncertainty
When there are many models with similar BIC, there could be problems to select the best. So, to be more sure about decision, I will use Bayes Factors.

Number of models by their number of predictors: `2^p` where p = number of predictors.


Construct probability distribution for all models (`2^p`) taking account of the following parameters:

* **prior = BIC**: that means that I will work with reference prior.
* **modelprior = uniform**: that means I will assin e same prior probability for all predictors.
```{r}
# Use `bas.lm` for regression
body_bas = bas.lm(Bodyfat ~ ., 
                 data = bodyfat, prior = "BIC",
                 modelprior = uniform())

# show all tools that I can use for body_bas
names(body_bas)
```

**BF**: Bayes Factors
```{r}
round(summary(body_bas), 3)
```
Showing the result, I can say the following:

* All top 5 models are agreed that Weight, Height, Neck, Hip, Thigh, Knee, Biceps, Forearm and Wrist should be excluded from the model, because they are zero.
* Model 1 and model 2 have the two highest posterior probabilities.

Calculate **marginal posterior inclusion probability** (pip) of each predictor.
```{r}
print(body_bas)
```

#### Visualize model uncertainty

Where:

* **y-axis**: predictors
* **x-axis**: models

Predictors with black color, are not include the model.
```{r}
image(body_bas, rotate = F)
```


```{r}
coef(body_bas)
```


```{r}
output = data.frame(model.size = body_bas$size, model.prob = body_bas$postprobs)

# Plot model size vs mode posterior probability
ggplot(data = output, aes(x = model.size, y = model.prob)) +
  geom_point(color = "blue", pch = 17, size = 3) +
  xlab("model size") + ylab("model posterior probability")
```


```{r}
2^14
```





